# -*- coding: utf-8 -*-
"""projectsmartsdle2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vAhPGArpg7D6ERg-ZrRk4ZaDon-8zSAH
"""

# Delete any existing SmartSDLC folders to avoid conflicts
!rm -rf smart_sdlc_backend smart_sdlc_frontend .env
print("ğŸ§¹ Cleaned old project folders.")

# Backend folders
!mkdir -p smart_sdlc_backend/app/routes
!mkdir -p smart_sdlc_backend/app/services
!mkdir -p smart_sdlc_backend/app/models
!mkdir -p smart_sdlc_backend/app/utils

# Frontend folder
!mkdir -p smart_sdlc_frontend/pages

print("ğŸ“ Project folders created.")

# Backend dependencies
!pip install fastapi uvicorn pymupdf python-dotenv langchain

# Frontend
!pip install streamlit streamlit-chat

# Watsonx SDK
!pip install ibm-watsonx-ai

# Extras
!pip install requests lottie

env_content = """
WATSONX_API_KEY=mocked_ibm_api_key_123
WATSONX_PROJECT_ID=mocked_project_id_456
WATSONX_MODEL_ID=granite-20b-code-instruct
HUGGINGFACE_TOKEN=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
"""

with open(".env", "w") as f:
    f.write(env_content)

print("âœ… .env file created.")

main_py_content = """
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.routes import ai_routes, auth_routes, chat_routes, feedback_routes

app = FastAPI(
    title="SmartSDLC - AI Enhanced SDLC",
    description="FastAPI backend for SmartSDLC",
    version="1.0.0"
)

# Allow frontend to communicate
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Route registration
app.include_router(ai_routes.router, prefix="/ai")
app.include_router(auth_routes.router, prefix="/auth")
app.include_router(chat_routes.router, prefix="/chat")
app.include_router(feedback_routes.router, prefix="/feedback")

@app.get("/")
def read_root():
    return {"message": "Welcome to SmartSDLC API!"}
"""

with open("smart_sdlc_backend/main.py", "w") as f:
    f.write(main_py_content)

print("âœ… main.py created.")

# Create SmartSDLC project folder structure
!rm -rf smart_sdlc_backend smart_sdlc_frontend .env  # Clean previous if exists
!mkdir -p smart_sdlc_backend/app/routes
!mkdir -p smart_sdlc_backend/app/services
!mkdir -p smart_sdlc_backend/app/models
!mkdir -p smart_sdlc_backend/app/utils
!mkdir -p smart_sdlc_frontend/pages

print("âœ… Folder structure created successfully.")

# Backend Dependencies
!pip install fastapi uvicorn pymupdf python-dotenv langchain

# Frontend Dependencies
!pip install streamlit streamlit-chat

# IBM Watsonx SDK
!pip install ibm-watsonx-ai

# Other useful tools
!pip install requests lottie

env_content = """
WATSONX_API_KEY=mocked_ibm_api_key_123
WATSONX_PROJECT_ID=mocked_project_id_456
WATSONX_MODEL_ID=granite-20b-code-instruct
HUGGINGFACE_TOKEN=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
"""

with open(".env", "w") as f:
    f.write(env_content.strip())

print("âœ… .env file created successfully.")

main_py_content = """
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.routes import ai_routes

app = FastAPI(
    title="SmartSDLC - AI Enhanced SDLC",
    description="FastAPI backend for SmartSDLC",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(ai_routes.router, prefix="/ai")

@app.get("/")
def read_root():
    return {"message": "Welcome to SmartSDLC API!"}
"""

with open("smart_sdlc_backend/main.py", "w") as f:
    f.write(main_py_content.strip())

print("âœ… main.py created successfully.")

ai_routes_code = """
from fastapi import APIRouter, UploadFile, File
from app.services.ai_story_generator import process_pdf_and_classify

router = APIRouter()

@router.post("/upload-pdf")
async def upload_pdf(file: UploadFile = File(...)):
    content = await file.read()
    result = process_pdf_and_classify(content)
    return {"status": "success", "data": result}
"""

with open("smart_sdlc_backend/app/routes/ai_routes.py", "w") as f:
    f.write(ai_routes_code.strip())

print("âœ… ai_routes.py created successfully.")

ai_story_generator_code = """
import fitz  # PyMuPDF

# TEMP MOCK â€” replace with Watsonx AI in later steps
def classify_sentence_with_ai(sentence):
    if "login" in sentence.lower():
        return "Requirements"
    elif "UI" in sentence or "button" in sentence:
        return "Design"
    elif "function" in sentence or "loop" in sentence:
        return "Development"
    elif "test" in sentence:
        return "Testing"
    elif "deploy" in sentence:
        return "Deployment"
    else:
        return "Uncategorized"

def process_pdf_and_classify(file_bytes):
    pdf_text = ""
    with fitz.open(stream=file_bytes, filetype="pdf") as doc:
        for page in doc:
            pdf_text += page.get_text()

    sentences = pdf_text.split(".")
    classified = {}

    for sentence in sentences:
        clean = sentence.strip()
        if clean:
            phase = classify_sentence_with_ai(clean)
            if phase not in classified:
                classified[phase] = []
            classified[phase].append(clean)

    return classified
"""

with open("smart_sdlc_backend/app/services/ai_story_generator.py", "w") as f:
    f.write(ai_story_generator_code.strip())

print("âœ… ai_story_generator.py created successfully.")

from google.colab import files

uploaded = files.upload()

from smart_sdlc_backend.app.services.ai_story_generator import process_pdf_and_classify
import json

# Get uploaded filename
filename = list(uploaded.keys())[0]

# Read file as bytes
with open(filename, "rb") as f:
    file_bytes = f.read()

# Call the classification function
result = process_pdf_and_classify(file_bytes)

# Show the result
print(json.dumps(result, indent=2))

code_generator_code = """
import sys
import os
sys.path.append(os.path.abspath("."))  # For Google Colab path resolution

from smart_sdlc_backend.app.services.watsonx_service import call_watsonx

async def generate_code_from_text(requirement_text: str):
    prompt = f\"\"\"Generate clean Python code for the following requirement:

{requirement_text}

Only return code without explanation.\"\"\"
    try:
        response = await call_watsonx(prompt)
        return {
            "status": "success",
            "code": response
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }
"""

with open("smart_sdlc_backend/app/services/code_generator.py", "w") as f:
    f.write(code_generator_code.strip())

print("âœ… code_generator.py created.")

import sys, os
sys.path.append(os.path.abspath("."))

watsonx_service_code = """
import os
import requests
from dotenv import load_dotenv

load_dotenv()

# Load Watsonx environment variables
API_KEY = os.getenv("WATSONX_API_KEY")
PROJECT_ID = os.getenv("WATSONX_PROJECT_ID")
MODEL_ID = os.getenv("WATSONX_MODEL_ID")

# Mock fallback if credentials are not set
if not API_KEY:
    API_KEY = "mocked_ibm_api_key_123"
    PROJECT_ID = "mocked_project_id_456"
    MODEL_ID = "granite-20b-code-instruct"

async def call_watsonx(prompt: str) -> str:
    # This is a mocked version for testing purpose
    if "factorial" in prompt:
        return "def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n - 1)"
    return "# AI generated code placeholder"
"""

with open("smart_sdlc_backend/app/services/watsonx_service.py", "w") as f:
    f.write(watsonx_service_code.strip())

print("âœ… watsonx_service.py fixed and created.")

import sys, os
sys.path.append(os.path.abspath("."))

import asyncio
from smart_sdlc_backend.app.services.code_generator import generate_code_from_text

requirement_text = "Create a function that calculates the factorial of a number using recursion."

async def test_code_generation():
    result = await generate_code_from_text(requirement_text)
    print("ğŸ§  Generated Code:")
    print(result)

await test_code_generation()

# âœ… Recreate bug_resolver.py
bug_resolver_code = """
import sys
import os
sys.path.append(os.path.abspath("."))  # Adds root directory to path

from smart_sdlc_backend.app.services.watsonx_service import call_watsonx

async def fix_buggy_code_service(buggy_code: str):
    prompt = f"Fix the following Python code:\\n\\n{buggy_code}\\n\\nReturn only the corrected code."
    try:
        response = await call_watsonx(prompt)
        return {
            "status": "success",
            "fixed_code": response
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }
"""

# Save file
os.makedirs("smart_sdlc_backend/app/services", exist_ok=True)
with open("smart_sdlc_backend/app/services/bug_resolver.py", "w") as f:
    f.write(bug_resolver_code)

print("âœ… bug_resolver.py recreated successfully.")

!ls smart_sdlc_backend/app/services

import sys
import os
sys.path.append(os.path.abspath("."))

import asyncio
from smart_sdlc_backend.app.services.bug_resolver import fix_buggy_code_service

buggy_code = "def greet(name):\n    pritn('Hello', name')"

async def test_bug_fix():
    result = await fix_buggy_code_service(buggy_code)
    print("ğŸ› ï¸ Bug Fix Result:")
    print(result)

await test_bug_fix()

import fitz  # PyMuPDF

# TEMP AI Classifier â€” Later this can be replaced with real AI like Watsonx
def classify_sentence_with_ai(sentence):
    sentence = sentence.lower()
    if "login" in sentence:
        return "Requirements"
    elif "ui" in sentence or "button" in sentence:
        return "Design"
    elif "function" in sentence or "loop" in sentence:
        return "Development"
    elif "test" in sentence:
        return "Testing"
    elif "deploy" in sentence:
        return "Deployment"
    else:
        return "Uncategorized"

def process_pdf_and_classify(file_bytes: bytes):
    pdf_text = ""
    with fitz.open(stream=file_bytes, filetype="pdf") as doc:
        for page in doc:
            pdf_text += page.get_text()

    sentences = [s.strip() for s in pdf_text.split(".") if s.strip()]
    classification = {}

    for sentence in sentences:
        phase = classify_sentence_with_ai(sentence)
        if phase not in classification:
            classification[phase] = []
        classification[phase].append(sentence)

    return classification

ai_story_generator_code = """
import fitz  # PyMuPDF

def classify_sentence_with_ai(sentence):
    sentence = sentence.lower()
    if "login" in sentence:
        return "Requirements"
    elif "ui" in sentence or "button" in sentence:
        return "Design"
    elif "function" in sentence or "loop" in sentence:
        return "Development"
    elif "test" in sentence:
        return "Testing"
    elif "deploy" in sentence:
        return "Deployment"
    else:
        return "Uncategorized"

def process_pdf_and_classify(file_bytes: bytes):
    pdf_text = ""
    with fitz.open(stream=file_bytes, filetype="pdf") as doc:
        for page in doc:
            pdf_text += page.get_text()

    sentences = [s.strip() for s in pdf_text.split(".") if s.strip()]
    classification = {}

    for sentence in sentences:
        phase = classify_sentence_with_ai(sentence)
        if phase not in classification:
            classification[phase] = []
        classification[phase].append(sentence)

    return classification
"""

with open("smart_sdlc_backend/app/services/ai_story_generator.py", "w") as f:
    f.write(ai_story_generator_code)

print("âœ… ai_story_generator.py created.")

from smart_sdlc_backend.app.services.ai_story_generator import process_pdf_and_classify
import json

filename = list(uploaded.keys())[0]

with open(filename, "rb") as f:
    file_bytes = f.read()

result = process_pdf_and_classify(file_bytes)

print("ğŸ“„ Classified Output:")
print(json.dumps(result, indent=2))

ai_routes_code = """
from fastapi import APIRouter, UploadFile, File
from app.services.ai_story_generator import process_pdf_and_classify

router = APIRouter()

@router.post("/upload-pdf")
async def upload_pdf(file: UploadFile = File(...)):
    content = await file.read()
    result = process_pdf_and_classify(content)
    return {"status": "success", "data": result}
"""

with open("smart_sdlc_backend/app/routes/ai_routes.py", "w") as f:
    f.write(ai_routes_code)

print("âœ… ai_routes.py created.")

import sys
import os

sys.path.append(os.path.abspath("smart_sdlc_backend"))
print("âœ… smart_sdlc_backend added to sys.path")

from app.routes import ai_routes

from smart_sdlc_backend.main import app
from fastapi.testclient import TestClient

client = TestClient(app)

pdf_filename = list(uploaded.keys())[0]

with open(pdf_filename, "rb") as f:
    response = client.post(
        "/ai/upload-pdf",
        files={"file": (pdf_filename, f, "application/pdf")}
    )

print("ğŸ“„ PDF Classification API Response:")
print(response.json())

import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="wide")

st.title("ğŸ“„ SmartSDLC - AI Enhanced SDLC")

uploaded_file = st.file_uploader("Upload a PDF document to classify software development phases", type=["pdf"])

if uploaded_file is not None:
    with st.spinner("Uploading and classifying..."):
        response = requests.post(
            "http://localhost:8000/ai/upload-pdf",  # backend must be running
            files={"file": uploaded_file.getvalue()}
        )

        if response.status_code == 200:
            result = response.json()
            st.success("âœ… PDF classified successfully!")
            st.json(result)
        else:
            st.error("âŒ Failed to classify PDF.")

with open("smart_sdlc_frontend/pages/Home.py", "w") as f:
    f.write("""<PASTE ABOVE CODE HERE>""")

frontend_home_code = """
import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="wide")

st.title("ğŸ“„ SmartSDLC - AI Enhanced SDLC")

uploaded_file = st.file_uploader("Upload a PDF document to classify software development phases", type=["pdf"])

if uploaded_file is not None:
    with st.spinner("Uploading and classifying..."):
        response = requests.post(
            "http://localhost:8000/ai/upload-pdf",
            files={"file": uploaded_file.getvalue()}
        )

        if response.status_code == 200:
            result = response.json()
            st.success("âœ… PDF classified successfully!")
            st.json(result)
        else:
            st.error("âŒ Failed to classify PDF.")
"""

with open("smart_sdlc_frontend/pages/Home.py", "w") as f:
    f.write(frontend_home_code)

print("âœ… Home.py for frontend created.")

!pip install pyngrok

!pip install fastapi uvicorn pyngrok python-multipart streamlit PyMuPDF

!pip install nest_asyncio

main_py_code = """
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from smart_sdlc_backend.app.routes import ai_routes
from smart_sdlc_backend.app.routes import auth_routes
from smart_sdlc_backend.app.routes import chat_routes
from smart_sdlc_backend.app.routes import feedback_routes

app = FastAPI(
    title="SmartSDLC - AI Enhanced SDLC",
    description="FastAPI backend for SmartSDLC",
    version="1.0.0"
)

# CORS Middleware for frontend interaction
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Route Inclusions
app.include_router(ai_routes.router, prefix="/ai")
app.include_router(auth_routes.router, prefix="/auth")
app.include_router(chat_routes.router, prefix="/chat")
app.include_router(feedback_routes.router, prefix="/feedback")

@app.get("/")
def read_root():
    return {"message": "Welcome to SmartSDLC API!"}
"""

with open("smart_sdlc_backend/main.py", "w") as f:
    f.write(main_py_code)

print("âœ… Fixed main.py imports")

ai_routes_fixed_code = """
from fastapi import APIRouter, UploadFile, File
from smart_sdlc_backend.app.services.ai_story_generator import process_pdf_and_classify

router = APIRouter()

@router.post("/upload-pdf")
async def upload_pdf(file: UploadFile = File(...)):
    content = await file.read()
    result = process_pdf_and_classify(content)
    return {"status": "success", "data": result}
"""

with open("smart_sdlc_backend/app/routes/ai_routes.py", "w") as f:
    f.write(ai_routes_fixed_code)

print("âœ… Fixed import in ai_routes.py")

!pip install fastapi uvicorn streamlit pyngrok

import nest_asyncio
import uvicorn
import asyncio

nest_asyncio.apply()

from fastapi import FastAPI

app = FastAPI()

# Optionally some route definitions below...

import sys
sys.path.append('/content/your_path_if_needed')  # Adjust if needed

from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "SmartSDLC API is running"}

# Create auth_routes.py with default content
auth_code = """
from fastapi import APIRouter

router = APIRouter()

@router.get("/auth/test")
def test_auth():
    return {"message": "Authentication route works"}
"""

with open("smart_sdlc_backend/app/routes/auth_routes.py", "w") as f:
    f.write(auth_code)

!touch smart_sdlc_backend/__init__.py
!touch smart_sdlc_backend/app/__init__.py
!touch smart_sdlc_backend/app/routes/__init__.py

chat_routes_code = """
from fastapi import APIRouter

router = APIRouter()

@router.get("/chat/test")
def test_chat():
    return {"message": "Chat route works"}
"""

with open("smart_sdlc_backend/app/routes/chat_routes.py", "w") as f:
    f.write(chat_routes_code)

feedback_routes_code = """
from fastapi import APIRouter

router = APIRouter()

@router.get("/feedback/test")
def test_feedback():
    return {"message": "Feedback route works"}
"""

with open("smart_sdlc_backend/app/routes/feedback_routes.py", "w") as f:
    f.write(feedback_routes_code)

from smart_sdlc_backend.main import app
import uvicorn

config = uvicorn.Config(app, host="0.0.0.0", port=8000, reload=True)
server = uvicorn.Server(config)

import threading
threading.Thread(target=server.run).start()

!pip install fastapi uvicorn streamlit pyngrok

from pyngrok import conf

# Set your valid Ngrok authtoken
conf.get_default().auth_token = "2z0XKNbwYraiv5tmL0FVNTaoxPe_83wh9cGRhwsQG2JzYL79V"

import streamlit as st
import requests
import os

# Set Hugging Face token
HF_TOKEN = "2z0XKNbwYraiv5tmL0FVNTaoxPe_83wh9cGRhwsQG2JzYL79V"
headers = {
    "Authorization": f"Bearer {HF_TOKEN}"
}

# Backend API URL (change this to your FastAPI's public URL via ngrok)
BACKEND_URL = "https://your-fastapi-ngrok-url.ngrok-free.app"

st.set_page_config(page_title="SmartSDLC", layout="centered")
st.title("ğŸ¤– SmartSDLC - AI Powered SDLC Platform")

menu = st.sidebar.radio("Choose Feature", ["ğŸ“„ Requirement Classifier", "ğŸ Bug Fixer", "ğŸ’¬ Feedback"])

# ğŸ“„ Requirement Classifier
if menu == "ğŸ“„ Requirement Classifier":
    st.subheader("ğŸ“„ Enter your software requirement")
    req_input = st.text_area("Requirement:", placeholder="e.g., The system should allow users to reset passwords")

    if st.button("Classify Requirement"):
        if req_input:
            with st.spinner("Classifying..."):
                response = requests.post(f"{BACKEND_URL}/ai/classify", json={"requirement": req_input})
                if response.status_code == 200:
                    result = response.json()
                    st.success(f"ğŸ“Œ Category: `{result['category']}`")
                else:
                    st.error("âŒ Failed to classify. Please check backend connection.")
        else:
            st.warning("Please enter a requirement first.")

# ğŸ Bug Fixer
elif menu == "ğŸ Bug Fixer":
    st.subheader("ğŸ Paste your buggy code")
    code_input = st.text_area("Code:", height=300)

    if st.button("Fix Bug"):
        if code_input:
            with st.spinner("Fixing bug..."):
                response = requests.post(f"{BACKEND_URL}/ai/fix-bug", json={"code": code_input})
                if response.status_code == 200:
                    result = response.json()
                    st.code(result["fixed_code"], language="python")
                else:
                    st.error("âŒ Failed to fix bug. Please check backend connection.")
        else:
            st.warning("Please enter code to fix.")

# ğŸ’¬ Feedback
elif menu == "ğŸ’¬ Feedback":
    st.subheader("ğŸ’¬ Submit Feedback")
    user = st.text_input("Your name or email (optional):")
    feedback = st.text_area("Your Feedback")

    if st.button("Submit Feedback"):
        if feedback.strip():
            response = requests.post(f"{BACKEND_URL}/feedback/submit", json={"user": user, "message": feedback})
            if response.status_code == 200:
                st.success("âœ… Feedback submitted successfully!")
            else:
                st.error("âŒ Submission failed. Check backend.")
        else:
            st.warning("Please write feedback before submitting.")

from pyngrok import ngrok
import os

# Kill any old tunnels
ngrok.kill()

# Path to your Streamlit app
streamlit_app_path = "smart_sdlc_frontend/pages/Home.py"

# Start a public tunnel on port 8501 (Streamlit default port)
public_url = ngrok.connect(8501)
print(f"ğŸŒ Streamlit App Public URL: {public_url}")

# Run Streamlit app
!streamlit run {streamlit_app_path} --server.port 8501 --server.headless true

!pkill streamlit

!uvicorn smart_sdlc_backend.main:app --host 0.0.0.0 --port 8000 --reload

!pip install pyngrok
from pyngrok import ngrok

# Set your new ngrok authtoken
ngrok.set_auth_token("2z0XKNbwYraiv5tmL0FVNTaoxPe_83wh9cGRhwsQG2JzYL79V")

# Kill existing tunnels
ngrok.kill()

# streamlit_app.py

import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="wide")
st.title("ğŸ¤– SmartSDLC - AI Enhanced Software Development Lifecycle")

# --- Requirement Classifier ---
st.header("ğŸ“„ Requirement Classifier")
req_text = st.text_area("Enter your software requirement")
if st.button("Classify Requirement"):
    if req_text.strip():
        response = requests.post("http://localhost:8000/classify", json={"requirement": req_text})
        if response.status_code == 200:
            st.success(f"Requirement Category: **{response.json()['category']}**")
        else:
            st.error("Error classifying requirement.")
    else:
        st.warning("Please enter some requirement text.")

# --- Code Generator ---
st.header("ğŸ’» Code Generator")
code_req = st.text_area("Describe the functionality you want to implement:")
if st.button("Generate Code"):
    if code_req.strip():
        response = requests.post("http://localhost:8000/generate_code", json={"requirement": code_req})
        if response.status_code == 200:
            st.code(response.json()["code"])
        else:
            st.error("Error generating code.")
    else:
        st.warning("Please enter a description.")

# --- Bug Fixer ---
st.header("ğŸ› ï¸ Bug Fixer")
buggy_code = st.text_area("Paste your buggy code:")
if st.button("Fix Bug"):
    if buggy_code.strip():
        response = requests.post("http://localhost:8000/fix_bug", json={"code": buggy_code})
        if response.status_code == 200:
            st.code(response.json()["fixed_code"])
        else:
            st.error("Error fixing code.")
    else:
        st.warning("Please paste some buggy code.")

!pkill -f ngrok
from pyngrok import ngrok

ngrok.kill()  # Ensure cleanup
ngrok.set_auth_token("2z0XKNbwYraiv5tmL0FVNTaoxPe_83wh9cGRhwsQG2JzYL79V")  # Set your token again

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py
# import streamlit as st
# import requests
# 
# st.set_page_config(page_title="SmartSDLC", layout="wide")
# st.title("ğŸ¤– SmartSDLC - AI Enhanced Software Development Lifecycle")
# 
# # --- Requirement Classifier ---
# st.header("ğŸ“„ Requirement Classifier")
# req_text = st.text_area("Enter your software requirement")
# if st.button("Classify Requirement"):
#     if req_text.strip():
#         response = requests.post("http://localhost:8000/classify", json={"requirement": req_text})
#         if response.status_code == 200:
#             st.success(f"Requirement Category: **{response.json()['category']}**")
#         else:
#             st.error("Error classifying requirement.")
#     else:
#         st.warning("Please enter some requirement text.")
# 
# # --- Code Generator ---
# st.header("ğŸ’» Code Generator")
# code_req = st.text_area("Describe the functionality you want to implement:")
# if st.button("Generate Code"):
#     if code_req.strip():
#         response = requests.post("http://localhost:8000/generate_code", json={"requirement": code_req})
#         if response.status_code == 200:
#             st.code(response.json()["code"])
#         else:
#             st.error("Error generating code.")
#     else:
#         st.warning("Please enter a description.")
# 
# # --- Bug Fixer ---
# st.header("ğŸ› ï¸ Bug Fixer")
# buggy_code = st.text_area("Paste your buggy code:")
# if st.button("Fix Bug"):
#     if buggy_code.strip():
#         response = requests.post("http://localhost:8000/fix_bug", json={"code": buggy_code})
#         if response.status_code == 200:
#             st.code(response.json()["fixed_code"])
#         else:
#             st.error("Error fixing code.")
#     else:
#         st.warning("Please paste some buggy code.")
#

!ls smart_sdlc_frontend/pages/

import streamlit as st
import requests

st.title("SmartSDLC Final Dashboard")

# Example: Requirement Classifier call
req = st.text_area("Enter Requirement")
if st.button("Classify"):
    res = requests.post("http://localhost:8000/classify", json={"requirement": req})
    st.write(res.json())

import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="centered")
st.title("ğŸ’¡ SmartSDLC Final Dashboard")

st.markdown("---")

# ğŸ”¹ Requirement Classifier
st.header("ğŸ“Œ Requirement Classifier")
req_input = st.text_area("Enter your requirement:")
if st.button("Classify Requirement"):
    if req_input.strip() == "":
        st.warning("Please enter a requirement.")
    else:
        try:
            res = requests.post("http://localhost:8000/classify", json={"requirement": req_input})
            st.success(f"Classification Result: {res.json()['classification']}")
        except:
            st.error("âŒ Backend service not running or unreachable.")

st.markdown("---")

# ğŸ”¹ Bug Fixer
st.header("ğŸ Bug Fixer AI")
bug_input = st.text_area("Paste your buggy code here:")
if st.button("Fix Bug"):
    if bug_input.strip() == "":
        st.warning("Please paste some buggy code.")
    else:
        try:
            res = requests.post("http://localhost:8000/fix-bugs", json={"code": bug_input})
            st.subheader("âœ… Fixed Code:")
            st.code(res.json()["fixed_code"], language="python")
        except:
            st.error("âŒ Backend service not running or unreachable.")

st.markdown("---")

# ğŸ”¹ Code Generator
st.header("âš™ï¸ Code Generator from Description")
desc_input = st.text_area("Describe the code you want to generate:")
if st.button("Generate Code"):
    if desc_input.strip() == "":
        st.warning("Please provide a description.")
    else:
        try:
            res = requests.post("http://localhost:8000/generate-code", json={"description": desc_input})
            st.subheader("ğŸš€ Generated Code:")
            st.code(res.json()["generated_code"], language="python")
        except:
            st.error("âŒ Backend service not running or unreachable.")

st.markdown("---")
st.info("âœ… All AI services integrated and ready!")

!streamlit run smart_sdlc_frontend/pages/Home.py --server.port 8501 --server.headless true

from pyngrok import ngrok
import os

# Kill any old tunnels
ngrok.kill()

# Step 1: Start ngrok tunnel
public_url = ngrok.connect(8501)
print("ğŸŒ Streamlit Public URL:", public_url)

# Step 2: Run your Streamlit app
!streamlit run smart_sdlc_frontend/pages/Home.py --server.port 8501 --server.headless true

!nohup uvicorn main:app --host 0.0.0.0 --port 8000 --reload &

from pyngrok import ngrok
import os

# Kill old tunnels
ngrok.kill()

# Start ngrok tunnel
public_url = ngrok.connect(8501)
print("ğŸŒ Streamlit Public URL:", public_url)

# Run Streamlit
!streamlit run smart_sdlc_frontend/pages/Home.py --server.port 8501 --server.headless true

!nohup uvicorn smart_sdlc_backend.main:app --host 0.0.0.0 --port 8000 --reload &

import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="centered")
st.title("ğŸš€ SmartSDLC - Final AI Dashboard")

st.write("Welcome! Choose a feature to use.")

# ğŸ§  Requirement Classifier
st.subheader("ğŸ§  Requirement Classifier")
req = st.text_area("Enter a software requirement")
if st.button("Classify Requirement"):
    res = requests.post("http://localhost:8000/classify", json={"requirement": req})
    st.json(res.json())

# ğŸª² Bug Fixer
st.subheader("ğŸª² Bug Fixer")
buggy_code = st.text_area("Paste your buggy code here")
if st.button("Fix Bug"):
    res = requests.post("http://localhost:8000/fix-bug", json={"code": buggy_code})
    st.code(res.json()["fixed_code"])

# ğŸ’» Code Generator
st.subheader("ğŸ’» Code Generator from Requirement")
code_req = st.text_input("Enter requirement for code generation")
if st.button("Generate Code"):
    res = requests.post("http://localhost:8000/generate-code", json={"requirement": code_req})
    st.code(res.json()["generated_code"])

from pyngrok import ngrok
import os

# Kill old tunnels
ngrok.kill()

# Set the correct file path
streamlit_app_path = "smart_sdlc_frontend/pages/Home.py"

# Start new public tunnel
public_url = ngrok.connect(8501)
print("ğŸŒ Streamlit App Public URL:", public_url)

# Run the Streamlit app
!streamlit run {streamlit_app_path} --server.port 8501 --server.headless true

!cat smart_sdlc_frontend/pages/Home.py

final_code = """
import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="centered")
st.title("ğŸš€ SmartSDLC - Final AI Dashboard")

st.write("Welcome! Choose a feature to use.")

# ğŸ§  Requirement Classifier
st.subheader("ğŸ§  Requirement Classifier")
req = st.text_area("Enter a software requirement")
if st.button("Classify Requirement"):
    res = requests.post("http://localhost:8000/classify", json={"requirement": req})
    st.json(res.json())

# ğŸª² Bug Fixer
st.subheader("ğŸª² Bug Fixer")
buggy_code = st.text_area("Paste your buggy code here")
if st.button("Fix Bug"):
    res = requests.post("http://localhost:8000/fix-bug", json={"code": buggy_code})
    st.code(res.json()["fixed_code"])

# ğŸ’» Code Generator
st.subheader("ğŸ’» Code Generator from Requirement")
code_req = st.text_input("Enter requirement for code generation")
if st.button("Generate Code"):
    res = requests.post("http://localhost:8000/generate-code", json={"requirement": code_req})
    st.code(res.json()["generated_code"])
"""

with open("smart_sdlc_frontend/pages/Home.py", "w") as f:
    f.write(final_code)

# Commented out IPython magic to ensure Python compatibility.
# %cd smart_sdlc_backend

# Fix the import error in main.py
file_path = "/content/smart_sdlc_backend/main.py"

with open(file_path, "r") as f:
    content = f.read()

# Replace incorrect import paths
content = content.replace(
    "from smart_sdlc_backend.app.routes import", "from app.routes import"
)

with open(file_path, "w") as f:
    f.write(content)

print("âœ… Fixed imports in main.py")

# Kill any existing Uvicorn servers that might be blocking the port
!pkill -f uvicorn || echo "No existing Uvicorn processes found"

import subprocess

# Run FastAPI backend in background
backend_process = subprocess.Popen(
    ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"],
    cwd="/content/smart_sdlc_backend",  # Update if your backend is in a different directory
)

print("âœ… FastAPI backend started in the background on port 8000")

# âœ… STEP 1: Save final Streamlit dashboard code to smart_sdlc_frontend/pages/Home.py

dashboard_code = """
import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="centered")
st.title("ğŸš€ SmartSDLC - Final AI Dashboard")

st.write("Welcome to SmartSDLC! Use the tools below:")

# ================================
# ğŸ§  Requirement Classifier
# ================================
st.subheader("ğŸ§  Requirement Classifier")
req_input = st.text_area("Enter a software requirement")
if st.button("Classify Requirement"):
    try:
        res = requests.post("http://localhost:8000/classify", json={"requirement": req_input})
        st.json(res.json())
    except:
        st.error("âŒ Failed to connect to backend. Is it running?")

# ================================
# ğŸ Bug Fixer
# ================================
st.subheader("ğŸª² Bug Fixer")
buggy_code = st.text_area("Paste your buggy code")
if st.button("Fix Bug"):
    try:
        res = requests.post("http://localhost:8000/fix-bug", json={"code": buggy_code})
        fixed = res.json().get("fixed_code", "âš ï¸ No code returned.")
        st.code(fixed)
    except:
        st.error("âŒ Failed to connect to backend. Is it running?")

# ================================
# ğŸ’» Code Generator
# ================================
st.subheader("ğŸ’» Code Generator from Requirement")
code_req = st.text_input("Enter a requirement to generate code")
if st.button("Generate Code"):
    try:
        res = requests.post("http://localhost:8000/generate-code", json={"requirement": code_req})
        generated = res.json().get("generated_code", "âš ï¸ No code returned.")
        st.code(generated)
    except:
        st.error("âŒ Failed to connect to backend. Is it running?")
"""

# Write the content to the file
with open('/content/smart_sdlc_frontend/pages/Home.py', 'w') as f:
    f.write(dashboard_code)

print("âœ… Final Home.py saved successfully.")

# Save final Home.py for backup/download
final_code = '''
import streamlit as st
import requests

st.set_page_config(page_title="SmartSDLC", layout="centered")
st.title("ğŸš€ SmartSDLC - Final AI Dashboard")

st.write("Welcome! Choose a feature to use.")

# ğŸ“Œ Requirement Classifier
st.subheader("ğŸ§  Requirement Classifier")
req = st.text_area("Enter a software requirement")
if st.button("Classify Requirement"):
    res = requests.post("http://localhost:8000/classify", json={"requirement": req})
    st.json(res.json())

# ğŸ“Œ Bug Fixer
st.subheader("ğŸª² Bug Fixer")
buggy_code = st.text_area("Paste your buggy code here")
if st.button("Fix Bug"):
    res = requests.post("http://localhost:8000/fix-bug", json={"code": buggy_code})
    st.code(res.json()["fixed_code"])

# ğŸ“Œ Code Generator
st.subheader("ğŸ’» Code Generator from Requirement")
code_req = st.text_input("Enter requirement for code generation")
if st.button("Generate Code"):
    res = requests.post("http://localhost:8000/generate-code", json={"requirement": code_req})
    st.code(res.json()["generated_code"])
'''

with open("/content/smart_sdlc_frontend/pages/Home.py", "w") as f:
    f.write(final_code)

print("âœ… Final Home.py saved successfully!")

# âœ… STEP 2: Launch Streamlit dashboard using ngrok tunnel

from pyngrok import ngrok
import os

# Kill old ngrok tunnels (to avoid "address already in use" error)
ngrok.kill()

# Set the path to your Home.py file
streamlit_app_path = "/content/smart_sdlc_frontend/pages/Home.py"

# Create a new public ngrok tunnel on port 8501 (Streamlit default)
public_url = ngrok.connect(8501)
print(f"ğŸŒ Public Dashboard URL: {public_url}")

# Run Streamlit app
!streamlit run {streamlit_app_path} --server.port 8501 --server.headless true